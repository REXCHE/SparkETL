
# Chapter 8 -> Spark ETL with Lakehouse | Apache HUDI

Task to do 
1. Read data from MySQL server into Spark
2. Create HIVE temp view from data frame
3. Load filtered data into HUDI format (create initial table)
4. Load filtered data again into HUDI format into same table 
5. Read HUDI tables using Spark data frame
6. Create Temp HIVE of HUDI tables
7. Write query to read data and also explore versions

Solution Notebook:<br/>
[Spark Notebook](chapter8.ipynb)

Blog with Explaination: <br/>
https://developershome.blog/2023/03/21/spark-etl-chapter-8-with-lakehouse-apache-hudi/

YouTube video with Explanation: <br/>
https://www.youtube.com/watch?v=eL1xIjranhg

Meduim Blog Channel: <br/>
https://medium.com/@developershome
