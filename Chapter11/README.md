
# Chapter 11 -> Spark ETL with Lakehouse | Delta table Optimization (Partition, ZORDER & Optimize)

Task to do 
1. Read data from MySQL server into Spark
2. Create HIVE temp view from data frame
3. Load filtered data into Delta format (create initial table)
4. Load filtered data into Delta format with partition 
5. Apply Optimize on delta table 
6. Apply ZOrder on delta table
7. Check performance

Solution Notebook:<br/>
[Spark Notebook](chapter9.ipynb)

Blog with Explaination: <br/>
https://developershome.blog/2023/03/21/spark-etl-chapter-9-with-lakehouse-apache-iceberg/

YouTube video with Explanation: <br/>
https://www.youtube.com/watch?v=eL1xIjranhg

Meduim Blog Channel: <br/>
https://medium.com/@developershome
