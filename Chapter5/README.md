
# Chapter 5 -> Spark ETL with Hive tables

Task to do 
1. Read data from one of the source (We take source as our MongoDB collection)
2. Create dataframe from source 
3. Create Hive table from dataframe
4. Create temp Hive view from dataframe
5. Create global Hive view from dataframe
6. List database and tables in database
7. Drop all the created tables and views in default database
8. Create Dataeng database and create global and temp view using SQL 
9. Access global table from other session


Solution Notebook:<br/>
[Spark Notebook](chapter5.ipynb)

Blog with Explaination: 

YouTube video with Explanation:
