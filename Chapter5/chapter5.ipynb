{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a51c7bc-a588-4d9e-bb74-a26148c92900",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Chapter 5 -> Spark ETL with Hive tables\n",
    "\n",
    "Task to do \n",
    "1. Read data from one of the source (We take source as our MongoDB collection)\n",
    "2. Create dataframe from source \n",
    "3. Create Hive table from dataframe\n",
    "4. Create temp Hive view from dataframe\n",
    "5. Create global Hive view from dataframe\n",
    "6. List database and tables in database\n",
    "7. Drop all the created tables and views in default database\n",
    "8. Create Dataeng database and create global and temp view using SQL \n",
    "9. Access global table from other session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea22d710-40f5-4b64-803e-83c583aa3472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Load all the required library and also Start Spark Session\n",
    "# Load all the required library\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368cc9c2-6f26-4d6b-93ff-d9ee7541869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-70a7e369-f076-4f97-9525-bde69055fef9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 335ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-70a7e369-f076-4f97-9525-bde69055fef9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/20ms)\n",
      "23/03/05 07:33:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#Start Spark Session\n",
    "spark = SparkSession.builder.appName(\"chapter5\")\\\n",
    "        .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1')\\\n",
    "        .getOrCreate()\n",
    "sqlContext = SparkSession(spark)\n",
    "#Dont Show warning only error\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2a3b43-6035-4104-94fb-d895cf2a524a",
   "metadata": {},
   "source": [
    "1. Read data from one of the source (We take source as our MongoDB collection)\n",
    "2. Create dataframe from source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42da13f2-c156-40e2-8ade-a55ff9d09f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://84d2b28f3cbe:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>chapter5</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f68a405d0a0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1682daf8-0610-44bc-bd5e-9ce5f3814e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodf = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", \"mongodb://root:mongodb@192.168.1.104:27017/\") \\\n",
    "    .option(\"database\", \"dataengineering\") \\\n",
    "    .option(\"collection\", \"employee\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3944d5d-0420-4b96-844d-b395886a13f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mongodf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3a9c513-d6e1-4e59-9e56-a12da2533375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{6402d551bd67c9b0...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{6402d551bd67c9b0...|         1006|      Todd|  1|   Wilson|106119|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mongodf.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd01eb-361e-4a2a-a1b1-7b880344085d",
   "metadata": {},
   "source": [
    "3. Create Hive table from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2447464e-c348-4628-b9fe-cb1bc0c7562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodf.write.saveAsTable(\"hivesampletable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f8a0c-c720-49c2-afaa-0fc22c4cc633",
   "metadata": {},
   "source": [
    "4. Create temp Hive view from dataframe\n",
    "5. Create global Hive view from dataframe\n",
    "\n",
    "The difference between temporary and global temporary views being subtle, it can be a source of mild confusion among developers new to Spark. A temporary view is tied to a single SparkSession within a Spark application. In contrast, a global temporary view is visible across multiple SparkSessions within a Spark application. Yes, you can create multiple SparkSessions within a single Spark application—this can be handy, for example, in cases where you want to access (and combine) data from two different SparkSessions that don’t share the same Hive metastore configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1410b71-4c59-46e7-bf79-e2d645921aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodf.createOrReplaceGlobalTempView(\"sampleglobalview\")\n",
    "mongodf.createOrReplaceTempView(\"sampletempview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b567d95f-dcf1-4083-875e-670b941b8cbb",
   "metadata": {},
   "source": [
    "6. List database and tables in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f74008c-d30e-4c77-9b20-e0895ed013e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  dataeng|\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42bf3cbd-fcbc-406b-bc24-7df63c038bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------+\n",
      "|namespace|      tableName|isTemporary|\n",
      "+---------+---------------+-----------+\n",
      "|  dataeng|hivesampletable|      false|\n",
      "|         | sampletempview|       true|\n",
      "+---------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f9788ab-4fe1-4964-a047-6a88ee7c0658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='dataeng', description='', locationUri='file:/opt/spark/SparkETL/Chapter5/spark-warehouse/dataeng.db'),\n",
       " Database(name='default', description='default database', locationUri='file:/opt/spark/SparkETL/Chapter5/spark-warehouse')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba7856c9-7ca1-4b54-84f2-c5f0f39b1db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='hivesampletable', database='dataeng', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='sampletempview', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb99c32d-bf12-4ae0-9254-7515824474d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='_id', description=None, dataType='struct<oid:string>', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='department_id', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='first_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='id', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='last_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='salary', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"hivesampletable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c757cc03-36f8-4c9f-8b88-e8a1c7485fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{6402d551bd67c9b0...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{6402d551bd67c9b0...|         1006|      Todd|  1|   Wilson|106119|\n",
      "|{6402d551bd67c9b0...|         1005|    Justin|  2|    Simon|128922|\n",
      "|{6402d551bd67c9b0...|         1005|    Justin|  2|    Simon|130000|\n",
      "|{6402d551bd67c9b0...|         1002|     Kelly|  3|  Rosario| 42689|\n",
      "|{6402d551bd67c9b0...|         1004|  Patricia|  4|   Powell|162825|\n",
      "|{6402d551bd67c9b0...|         1004|  Patricia|  4|   Powell|170000|\n",
      "|{6402d551bd67c9b0...|         1002|    Sherry|  5|   Golden| 44101|\n",
      "|{6402d551bd67c9b0...|         1005|   Natasha|  6|  Swanson| 79632|\n",
      "|{6402d551bd67c9b0...|         1005|   Natasha|  6|  Swanson| 90000|\n",
      "|{6402d551bd67c9b0...|         1002|     Diane|  7|   Gordon| 74591|\n",
      "|{6402d551bd67c9b0...|         1005|  Mercedes|  8|Rodriguez| 61048|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|137236|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|140000|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|150000|\n",
      "|{6402d551bd67c9b0...|         1006|      Sean| 10| Crawford|182065|\n",
      "|{6402d551bd67c9b0...|         1006|      Sean| 10| Crawford|190000|\n",
      "|{6402d551bd67c9b0...|         1002|     Kevin| 11| Townsend|166861|\n",
      "|{6402d551bd67c9b0...|         1004|    Joshua| 12|  Johnson|123082|\n",
      "|{6402d551bd67c9b0...|         1001|     Julie| 13|  Sanchez|185663|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM hivesampletable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fb5fafe-af29-49ed-b88a-99913818d940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{6402d551bd67c9b0...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{6402d551bd67c9b0...|         1006|      Todd|  1|   Wilson|106119|\n",
      "|{6402d551bd67c9b0...|         1005|    Justin|  2|    Simon|128922|\n",
      "|{6402d551bd67c9b0...|         1005|    Justin|  2|    Simon|130000|\n",
      "|{6402d551bd67c9b0...|         1002|     Kelly|  3|  Rosario| 42689|\n",
      "|{6402d551bd67c9b0...|         1004|  Patricia|  4|   Powell|162825|\n",
      "|{6402d551bd67c9b0...|         1004|  Patricia|  4|   Powell|170000|\n",
      "|{6402d551bd67c9b0...|         1002|    Sherry|  5|   Golden| 44101|\n",
      "|{6402d551bd67c9b0...|         1005|   Natasha|  6|  Swanson| 79632|\n",
      "|{6402d551bd67c9b0...|         1005|   Natasha|  6|  Swanson| 90000|\n",
      "|{6402d551bd67c9b0...|         1002|     Diane|  7|   Gordon| 74591|\n",
      "|{6402d551bd67c9b0...|         1005|  Mercedes|  8|Rodriguez| 61048|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|137236|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|140000|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|150000|\n",
      "|{6402d551bd67c9b0...|         1006|      Sean| 10| Crawford|182065|\n",
      "|{6402d551bd67c9b0...|         1006|      Sean| 10| Crawford|190000|\n",
      "|{6402d551bd67c9b0...|         1002|     Kevin| 11| Townsend|166861|\n",
      "|{6402d551bd67c9b0...|         1004|    Joshua| 12|  Johnson|123082|\n",
      "|{6402d551bd67c9b0...|         1001|     Julie| 13|  Sanchez|185663|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"sampletempview\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7902e3d4-9c4e-4553-94bf-9dbc0dbff4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{6402d551bd67c9b0...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{6402d551bd67c9b0...|         1006|      Todd|  1|   Wilson|106119|\n",
      "|{6402d551bd67c9b0...|         1005|    Justin|  2|    Simon|128922|\n",
      "|{6402d551bd67c9b0...|         1005|    Justin|  2|    Simon|130000|\n",
      "|{6402d551bd67c9b0...|         1002|     Kelly|  3|  Rosario| 42689|\n",
      "|{6402d551bd67c9b0...|         1004|  Patricia|  4|   Powell|162825|\n",
      "|{6402d551bd67c9b0...|         1004|  Patricia|  4|   Powell|170000|\n",
      "|{6402d551bd67c9b0...|         1002|    Sherry|  5|   Golden| 44101|\n",
      "|{6402d551bd67c9b0...|         1005|   Natasha|  6|  Swanson| 79632|\n",
      "|{6402d551bd67c9b0...|         1005|   Natasha|  6|  Swanson| 90000|\n",
      "|{6402d551bd67c9b0...|         1002|     Diane|  7|   Gordon| 74591|\n",
      "|{6402d551bd67c9b0...|         1005|  Mercedes|  8|Rodriguez| 61048|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|137236|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|140000|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|150000|\n",
      "|{6402d551bd67c9b0...|         1006|      Sean| 10| Crawford|182065|\n",
      "|{6402d551bd67c9b0...|         1006|      Sean| 10| Crawford|190000|\n",
      "|{6402d551bd67c9b0...|         1002|     Kevin| 11| Townsend|166861|\n",
      "|{6402d551bd67c9b0...|         1004|    Joshua| 12|  Johnson|123082|\n",
      "|{6402d551bd67c9b0...|         1001|     Julie| 13|  Sanchez|185663|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM global_temp.sampleglobalview\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c0bc5-e3e8-4ac3-94f3-d0e119e31f23",
   "metadata": {},
   "source": [
    "7. Drop all the created tables and views in default database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e30ad8e-7ad8-4c79-b197-41719d21c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropGlobalTempView(\"sampleglobalview\")\n",
    "spark.catalog.dropTempView(\"sampletempview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eba5f1-9fd8-4d92-97b6-638250d475e8",
   "metadata": {},
   "source": [
    "8. Create Dataeng database and create global and temp view using SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5644d360-38b3-4a6e-867d-5a86772d268c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE dataeng\")\n",
    "spark.sql(\"USE dataeng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36c017c5-efb4-4a20-9628-e4bcb1a2e068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  dataeng|\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2dcdebaf-ad5d-4fa9-9ec2-49903c734386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------+\n",
      "|namespace|      tableName|isTemporary|\n",
      "+---------+---------------+-----------+\n",
      "|  dataeng|hivesampletable|      false|\n",
      "|         | sampletempview|       true|\n",
      "+---------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ae7b89d-d5a8-488e-af07-7b4e2295af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodf.createOrReplaceTempView(\"sampletempview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c741fd-d170-4efd-963d-20d917872e95",
   "metadata": {},
   "source": [
    "9. Access global table from other session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2166405a-0b3d-427d-9ebd-35d03172397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newSpark = spark.newSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bbb3b2bc-0a4d-467b-ad53-04df44944db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://84d2b28f3cbe:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>chapter5</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6898e5a6d0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d170beb-4996-4c40-9374-bd0ba6747b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{6402d551bd67c9b0...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{6402d551bd67c9b0...|         1006|      Todd|  1|   Wilson|106119|\n",
      "|{6402d551bd67c9b0...|         1005|    Justin|  2|    Simon|128922|\n",
      "|{6402d551bd67c9b0...|         1005|    Justin|  2|    Simon|130000|\n",
      "|{6402d551bd67c9b0...|         1002|     Kelly|  3|  Rosario| 42689|\n",
      "|{6402d551bd67c9b0...|         1004|  Patricia|  4|   Powell|162825|\n",
      "|{6402d551bd67c9b0...|         1004|  Patricia|  4|   Powell|170000|\n",
      "|{6402d551bd67c9b0...|         1002|    Sherry|  5|   Golden| 44101|\n",
      "|{6402d551bd67c9b0...|         1005|   Natasha|  6|  Swanson| 79632|\n",
      "|{6402d551bd67c9b0...|         1005|   Natasha|  6|  Swanson| 90000|\n",
      "|{6402d551bd67c9b0...|         1002|     Diane|  7|   Gordon| 74591|\n",
      "|{6402d551bd67c9b0...|         1005|  Mercedes|  8|Rodriguez| 61048|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|137236|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|140000|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|150000|\n",
      "|{6402d551bd67c9b0...|         1006|      Sean| 10| Crawford|182065|\n",
      "|{6402d551bd67c9b0...|         1006|      Sean| 10| Crawford|190000|\n",
      "|{6402d551bd67c9b0...|         1002|     Kevin| 11| Townsend|166861|\n",
      "|{6402d551bd67c9b0...|         1004|    Joshua| 12|  Johnson|123082|\n",
      "|{6402d551bd67c9b0...|         1001|     Julie| 13|  Sanchez|185663|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newSpark.sql(\"SELECT * FROM global_temp.sampleglobalview\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd67ca-8ced-4ddd-a47d-da71d5b2e626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
