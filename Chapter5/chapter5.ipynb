{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a51c7bc-a588-4d9e-bb74-a26148c92900",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Chapter 5 -> Spark ETL with Hive tables\n",
    "\n",
    "Task to do \n",
    "1. Read data from one of the source (We take source as our MongoDB collection)\n",
    "2. Create dataframe from source \n",
    "3. Create Hive table from dataframe\n",
    "4. Create temp Hive view from dataframe\n",
    "5. Create global Hive view from dataframe\n",
    "6. List database and tables in database\n",
    "7. Drop all the created tables and views in default database\n",
    "8. Create Dataeng database and create global and temp view using SQL \n",
    "9. Access global table from other session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea22d710-40f5-4b64-803e-83c583aa3472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Load all the required library and also Start Spark Session\n",
    "# Load all the required library\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368cc9c2-6f26-4d6b-93ff-d9ee7541869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f2ed49bd-7bd8-4327-8579-ed71257d3bbb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 830ms :: artifacts dl 128ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f2ed49bd-7bd8-4327-8579-ed71257d3bbb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/30ms)\n",
      "23/03/15 07:51:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#Start Spark Session\n",
    "spark = SparkSession.builder.appName(\"chapter5\")\\\n",
    "        .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1')\\\n",
    "        .getOrCreate()\n",
    "sqlContext = SparkSession(spark)\n",
    "#Dont Show warning only error\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2a3b43-6035-4104-94fb-d895cf2a524a",
   "metadata": {},
   "source": [
    "1. Read data from one of the source (We take source as our MongoDB collection)\n",
    "2. Create dataframe from source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42da13f2-c156-40e2-8ade-a55ff9d09f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://84d2b28f3cbe:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>chapter5</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f68a405d0a0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1682daf8-0610-44bc-bd5e-9ce5f3814e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mongodf = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", \"mongodb://root:mongodb@192.168.1.104:27017/\") \\\n",
    "    .option(\"database\", \"dataengineering\") \\\n",
    "    .option(\"collection\", \"employee\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3944d5d-0420-4b96-844d-b395886a13f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mongodf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a9c513-d6e1-4e59-9e56-a12da2533375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{6407c350840f10d7...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{6407c350840f10d7...|         1006|      Todd|  1|   Wilson|106119|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mongodf.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd01eb-361e-4a2a-a1b1-7b880344085d",
   "metadata": {},
   "source": [
    "3. Create Hive table from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2447464e-c348-4628-b9fe-cb1bc0c7562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mongodf.write.saveAsTable(\"hivesampletable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f8a0c-c720-49c2-afaa-0fc22c4cc633",
   "metadata": {},
   "source": [
    "4. Create temp Hive view from dataframe\n",
    "5. Create global Hive view from dataframe\n",
    "\n",
    "The difference between temporary and global temporary views being subtle, it can be a source of mild confusion among developers new to Spark. A temporary view is tied to a single SparkSession within a Spark application. In contrast, a global temporary view is visible across multiple SparkSessions within a Spark application. Yes, you can create multiple SparkSessions within a single Spark application—this can be handy, for example, in cases where you want to access (and combine) data from two different SparkSessions that don’t share the same Hive metastore configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1410b71-4c59-46e7-bf79-e2d645921aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodf.createOrReplaceGlobalTempView(\"sampleglobalview\")\n",
    "mongodf.createOrReplaceTempView(\"sampletempview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b567d95f-dcf1-4083-875e-670b941b8cbb",
   "metadata": {},
   "source": [
    "6. List database and tables in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f74008c-d30e-4c77-9b20-e0895ed013e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42bf3cbd-fcbc-406b-bc24-7df63c038bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------+\n",
      "|namespace|      tableName|isTemporary|\n",
      "+---------+---------------+-----------+\n",
      "|  default|hivesampletable|      false|\n",
      "|         | sampletempview|       true|\n",
      "+---------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f9788ab-4fe1-4964-a047-6a88ee7c0658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/opt/spark/SparkETL/Chapter5/spark-warehouse')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba7856c9-7ca1-4b54-84f2-c5f0f39b1db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='hivesampletable', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='sampletempview', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb99c32d-bf12-4ae0-9254-7515824474d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='_id', description=None, dataType='struct<oid:string>', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='department_id', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='first_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='id', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='last_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='salary', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"hivesampletable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c757cc03-36f8-4c9f-8b88-e8a1c7485fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{6407c350840f10d7...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{6407c350840f10d7...|         1006|      Todd|  1|   Wilson|106119|\n",
      "|{6407c350840f10d7...|         1005|    Justin|  2|    Simon|128922|\n",
      "|{6407c350840f10d7...|         1005|    Justin|  2|    Simon|130000|\n",
      "|{6407c350840f10d7...|         1002|     Kelly|  3|  Rosario| 42689|\n",
      "|{6407c350840f10d7...|         1004|  Patricia|  4|   Powell|162825|\n",
      "|{6407c350840f10d7...|         1004|  Patricia|  4|   Powell|170000|\n",
      "|{6407c350840f10d7...|         1002|    Sherry|  5|   Golden| 44101|\n",
      "|{6407c350840f10d7...|         1005|   Natasha|  6|  Swanson| 79632|\n",
      "|{6407c350840f10d7...|         1005|   Natasha|  6|  Swanson| 90000|\n",
      "|{6407c350840f10d7...|         1002|     Diane|  7|   Gordon| 74591|\n",
      "|{6407c350840f10d7...|         1005|  Mercedes|  8|Rodriguez| 61048|\n",
      "|{6407c350840f10d7...|         1001|   Christy|  9| Mitchell|137236|\n",
      "|{6407c350840f10d7...|         1001|   Christy|  9| Mitchell|140000|\n",
      "|{6407c350840f10d7...|         1001|   Christy|  9| Mitchell|150000|\n",
      "|{6407c350840f10d7...|         1006|      Sean| 10| Crawford|182065|\n",
      "|{6407c350840f10d7...|         1006|      Sean| 10| Crawford|190000|\n",
      "|{6407c350840f10d7...|         1002|     Kevin| 11| Townsend|166861|\n",
      "|{6407c350840f10d7...|         1004|    Joshua| 12|  Johnson|123082|\n",
      "|{6407c350840f10d7...|         1001|     Julie| 13|  Sanchez|185663|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM hivesampletable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fb5fafe-af29-49ed-b88a-99913818d940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{6402d551bd67c9b0...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{6402d551bd67c9b0...|         1006|      Todd|  1|   Wilson|106119|\n",
      "|{6402d551bd67c9b0...|         1005|    Justin|  2|    Simon|128922|\n",
      "|{6402d551bd67c9b0...|         1005|    Justin|  2|    Simon|130000|\n",
      "|{6402d551bd67c9b0...|         1002|     Kelly|  3|  Rosario| 42689|\n",
      "|{6402d551bd67c9b0...|         1004|  Patricia|  4|   Powell|162825|\n",
      "|{6402d551bd67c9b0...|         1004|  Patricia|  4|   Powell|170000|\n",
      "|{6402d551bd67c9b0...|         1002|    Sherry|  5|   Golden| 44101|\n",
      "|{6402d551bd67c9b0...|         1005|   Natasha|  6|  Swanson| 79632|\n",
      "|{6402d551bd67c9b0...|         1005|   Natasha|  6|  Swanson| 90000|\n",
      "|{6402d551bd67c9b0...|         1002|     Diane|  7|   Gordon| 74591|\n",
      "|{6402d551bd67c9b0...|         1005|  Mercedes|  8|Rodriguez| 61048|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|137236|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|140000|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|150000|\n",
      "|{6402d551bd67c9b0...|         1006|      Sean| 10| Crawford|182065|\n",
      "|{6402d551bd67c9b0...|         1006|      Sean| 10| Crawford|190000|\n",
      "|{6402d551bd67c9b0...|         1002|     Kevin| 11| Townsend|166861|\n",
      "|{6402d551bd67c9b0...|         1004|    Joshua| 12|  Johnson|123082|\n",
      "|{6402d551bd67c9b0...|         1001|     Julie| 13|  Sanchez|185663|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"sampletempview\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7902e3d4-9c4e-4553-94bf-9dbc0dbff4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{6407c350840f10d7...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{6407c350840f10d7...|         1006|      Todd|  1|   Wilson|106119|\n",
      "|{6407c350840f10d7...|         1005|    Justin|  2|    Simon|128922|\n",
      "|{6407c350840f10d7...|         1005|    Justin|  2|    Simon|130000|\n",
      "|{6407c350840f10d7...|         1002|     Kelly|  3|  Rosario| 42689|\n",
      "|{6407c350840f10d7...|         1004|  Patricia|  4|   Powell|162825|\n",
      "|{6407c350840f10d7...|         1004|  Patricia|  4|   Powell|170000|\n",
      "|{6407c350840f10d7...|         1002|    Sherry|  5|   Golden| 44101|\n",
      "|{6407c350840f10d7...|         1005|   Natasha|  6|  Swanson| 79632|\n",
      "|{6407c350840f10d7...|         1005|   Natasha|  6|  Swanson| 90000|\n",
      "|{6407c350840f10d7...|         1002|     Diane|  7|   Gordon| 74591|\n",
      "|{6407c350840f10d7...|         1005|  Mercedes|  8|Rodriguez| 61048|\n",
      "|{6407c350840f10d7...|         1001|   Christy|  9| Mitchell|137236|\n",
      "|{6407c350840f10d7...|         1001|   Christy|  9| Mitchell|140000|\n",
      "|{6407c350840f10d7...|         1001|   Christy|  9| Mitchell|150000|\n",
      "|{6407c350840f10d7...|         1006|      Sean| 10| Crawford|182065|\n",
      "|{6407c350840f10d7...|         1006|      Sean| 10| Crawford|190000|\n",
      "|{6407c350840f10d7...|         1002|     Kevin| 11| Townsend|166861|\n",
      "|{6407c350840f10d7...|         1004|    Joshua| 12|  Johnson|123082|\n",
      "|{6407c350840f10d7...|         1001|     Julie| 13|  Sanchez|185663|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM global_temp.sampleglobalview\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c0bc5-e3e8-4ac3-94f3-d0e119e31f23",
   "metadata": {},
   "source": [
    "7. Drop all the created tables and views in default database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e30ad8e-7ad8-4c79-b197-41719d21c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropGlobalTempView(\"sampleglobalview\")\n",
    "spark.catalog.dropTempView(\"sampletempview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eba5f1-9fd8-4d92-97b6-638250d475e8",
   "metadata": {},
   "source": [
    "8. Create Dataeng database and create global and temp view using SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5644d360-38b3-4a6e-867d-5a86772d268c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE dataeng\")\n",
    "spark.sql(\"USE dataeng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36c017c5-efb4-4a20-9628-e4bcb1a2e068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  dataeng|\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44082455-4774-41ff-b943-16a315b661f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodf.write.saveAsTable(\"hivesampletable\")\n",
    "mongodf.createOrReplaceGlobalTempView(\"sampleglobalview\")\n",
    "mongodf.createOrReplaceTempView(\"sampletempview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dcdebaf-ad5d-4fa9-9ec2-49903c734386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------+\n",
      "|namespace|      tableName|isTemporary|\n",
      "+---------+---------------+-----------+\n",
      "|  dataeng|hivesampletable|      false|\n",
      "|         | sampletempview|       true|\n",
      "+---------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c741fd-d170-4efd-963d-20d917872e95",
   "metadata": {},
   "source": [
    "9. Access global table from other session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2166405a-0b3d-427d-9ebd-35d03172397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newSpark = spark.newSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbb3b2bc-0a4d-467b-ad53-04df44944db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://84d2b28f3cbe:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>chapter5</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f56702544c0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d170beb-4996-4c40-9374-bd0ba6747b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|                 _id|department_id|first_name| id|last_name|salary|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "|{6402d551bd67c9b0...|         1006|      Todd|  1|   Wilson|110000|\n",
      "|{6402d551bd67c9b0...|         1006|      Todd|  1|   Wilson|106119|\n",
      "|{6402d551bd67c9b0...|         1005|    Justin|  2|    Simon|128922|\n",
      "|{6402d551bd67c9b0...|         1005|    Justin|  2|    Simon|130000|\n",
      "|{6402d551bd67c9b0...|         1002|     Kelly|  3|  Rosario| 42689|\n",
      "|{6402d551bd67c9b0...|         1004|  Patricia|  4|   Powell|162825|\n",
      "|{6402d551bd67c9b0...|         1004|  Patricia|  4|   Powell|170000|\n",
      "|{6402d551bd67c9b0...|         1002|    Sherry|  5|   Golden| 44101|\n",
      "|{6402d551bd67c9b0...|         1005|   Natasha|  6|  Swanson| 79632|\n",
      "|{6402d551bd67c9b0...|         1005|   Natasha|  6|  Swanson| 90000|\n",
      "|{6402d551bd67c9b0...|         1002|     Diane|  7|   Gordon| 74591|\n",
      "|{6402d551bd67c9b0...|         1005|  Mercedes|  8|Rodriguez| 61048|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|137236|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|140000|\n",
      "|{6402d551bd67c9b0...|         1001|   Christy|  9| Mitchell|150000|\n",
      "|{6402d551bd67c9b0...|         1006|      Sean| 10| Crawford|182065|\n",
      "|{6402d551bd67c9b0...|         1006|      Sean| 10| Crawford|190000|\n",
      "|{6402d551bd67c9b0...|         1002|     Kevin| 11| Townsend|166861|\n",
      "|{6402d551bd67c9b0...|         1004|    Joshua| 12|  Johnson|123082|\n",
      "|{6402d551bd67c9b0...|         1001|     Julie| 13|  Sanchez|185663|\n",
      "+--------------------+-------------+----------+---+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newSpark.sql(\"SELECT * FROM global_temp.sampleglobalview\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2fd67ca-8ced-4ddd-a47d-da71d5b2e626",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: sampletempview; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [sampletempview], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnewSpark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM sampletempview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: sampletempview; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [sampletempview], [], false\n"
     ]
    }
   ],
   "source": [
    "newSpark.sql(\"SELECT * FROM sampletempview\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067b9a71-0dea-457f-812d-557242af506e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
